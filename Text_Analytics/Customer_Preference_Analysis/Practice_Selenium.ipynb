{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium library\n",
    "**Try it out**\n",
    "\n",
    "This jupyter notebook is designed to take you through the `selenium` library in Python, which is great at controlling your default web browser (usually Chrome) and scraping whatever you fancy off the internet. Be careful though, do NOT scrape websites that don't allow it. Your IP could forever be banned from visiting that website again.\n",
    "\n",
    "Here, we make an attempt to scrape some data off of Craigslist. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note -\n",
    "\n",
    "1. I am using the Chrome web browser while doing this and I assume you have Chrome. \n",
    "\n",
    "2. You will need the chrome webdrive. Download it here - \n",
    "http://chromedriver.chromium.org/downloads\n",
    "\n",
    "Extract the downloaded file on your local machine. I have extracted it to my Desktop so it's easy to access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lmtzr = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample Problem\n",
    "\n",
    "Let's say I want to navigate to the website \"https://vancouver.craigslist.ca\" and look at all the articles on offer under the antiques section but only using code. How would I do that?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1:** *Fire up the chrome webdriver and navigate to the website*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# provide location to chromedriver and open chrome\n",
    "chrome_path = r'C:/Users/sagar/Desktop/chromedriver'\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "\n",
    "# open the craiglist website\n",
    "driver.get(\"https://vancouver.craigslist.ca\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After **Step 1**, a new chrome window would have opened up and loaded the cragslist webpage for you. Press F12. Then locate the 'Antiques' link, right click on it and say inspect. This would highlight in blue some text on the right window pane. Right click on that and select `Copy ->Copy XPath`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](ScreenCapture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2:** *Instruct Chrome to click on the Antiques link*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a link we want to open, paste the copied XPath here within triple quotes.\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"sss0\"]/li[1]/a/span\"\"\").click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3:** *Right click the link for any product and select inspect.* <br>\n",
    "A new window pane will open and some text will be highlighted on it. In situations where we have many links of one type, it is better to work with the `find_elements_by_class_name`. This will find all similar links on the page. <br>\n",
    "\n",
    "In this case, we see that within the text it says `class = result-title hdrlink`. Let's use this aspect to find all product names on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on the webpage, find all elements with this class and return their text\n",
    "posts = driver.find_elements_by_class_name('hdrlnk')\n",
    "\n",
    "post_text = []\n",
    "for post in posts:\n",
    "    post_text.append(post.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hurray! In the `post_text` variable, we have succesfully scraped all product names from the craigslist page! WELL DONE!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we want all links from all pages in craigslist?\n",
    "1. This time let's use the Automotive link under the services tab.\n",
    "2. For this link, we get a lot of 'next' buttons on the top.\n",
    "3. We want to capture all the links on all the pages returned by the main link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open a website\n",
    "driver = webdriver.Chrome(chrome_path)\n",
    "driver.get(\"https://vancouver.craigslist.ca\")\n",
    "\n",
    "## Click on the page for Automotive services. Find the XPath as described above!\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"bbb0\"]/li[1]/a/span\"\"\").click()\n",
    "\n",
    "# get header links for these services and add to the post texts. Same as what we did for antique products.\n",
    "auto_posts = driver.find_elements_by_class_name('hdrlnk')\n",
    "auto_post_text = []\n",
    "\n",
    "for post in auto_posts:\n",
    "    auto_post_text.append(post.text)\n",
    "\n",
    "# start a loop and keep looking for the next link until error encountered\n",
    "keep_searching = True\n",
    "\n",
    "while keep_searching:\n",
    "    try:\n",
    "        # this is the XPath for the 'Next' button\n",
    "        driver.find_element_by_xpath(\"\"\"//*[@id=\"searchform\"]/div[3]/div[3]/span[2]/a[3]\"\"\").click()\n",
    "        auto_posts = driver.find_elements_by_class_name('hdrlnk')\n",
    "        for post in auto_posts:\n",
    "            auto_post_text.append(post.text)\n",
    "    except:\n",
    "        keep_searching = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do some processing with this text as well. Let's say we have to find the most frequent words used by ad posters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'car': 91, 'towing': 71, 'scrap': 63, 'cash': 56, 'mobile': 54, 'removal': 53, 'service': 46, 'auto': 38, 'mechanic': 37, '247': 31, ...})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# most commonly occuring words in the text extracted... remove punctuations and stopwords\n",
    "auto_post_string = \" \".join(auto_post_text)\n",
    "for p in punctuation:\n",
    "    auto_post_string = auto_post_string.lower().replace(p, \"\")\n",
    "    \n",
    "en_stopwords = stopwords.words('english')\n",
    "tokens = word_tokenize(auto_post_string)\n",
    "\n",
    "tokens_no_stopwords = [w for w in tokens if w not in en_stopwords]\n",
    "nltk.FreqDist(tokens_no_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's say for each automotive service listed, we want the whole text on the page. What do we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the craigslist website\n",
    "driver.get(\"https://vancouver.craigslist.ca\")\n",
    "\n",
    "## Click on the page for Automotive services\n",
    "driver.find_element_by_xpath(\"\"\"//*[@id=\"bbb0\"]/li[1]/a/span\"\"\").click()\n",
    "\n",
    "# get header links for a service and click on it\n",
    "driver.find_element_by_class_name('hdrlnk').click()\n",
    "\n",
    "# get the body of the post\n",
    "comp_text = driver.find_elements_by_id('postingbody')\n",
    "full_ad = []\n",
    "for text in comp_text:\n",
    "    full_ad.append(text.text)\n",
    "\n",
    "# keep looking for the 'next' button and keep extracting the body of the text. Stop if next not found.\n",
    "keep_searching = True\n",
    "while keep_searching:\n",
    "    try:\n",
    "        driver.find_element_by_xpath(\"\"\"/html/body/section/section/header/div[1]/div/a[3]\"\"\").click()\n",
    "        comp_text = driver.find_elements_by_id('postingbody')\n",
    "        for text in comp_text:\n",
    "            full_ad.append(text.text)\n",
    "    except:\n",
    "        keep_searching = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'car': 148478, 'cash': 123790, 'clunkers': 59740, 'junk': 56030, 'vehicle': 48191, 'auto': 43969, 'scrap': 40291, 'truck': 36154, 'towing': 36095, 'sell': 35872, ...})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = \" \".join(full_ad)\n",
    "for p in punctuation:\n",
    "    corpus = corpus.lower().replace(p, \"\")\n",
    "    \n",
    "en_stopwords = stopwords.words('english')\n",
    "tokens = word_tokenize(corpus)\n",
    "\n",
    "tokens_no_stopwords = [w for w in tokens if w not in en_stopwords]\n",
    "tokens_nostop_lemm = [lmtzr.lemmatize(w) for w in tokens_no_stopwords]\n",
    "nltk.FreqDist(tokens_nostop_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There you go!\n",
    "You can now do many cool things with Selenium. I will use some of these in my analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
