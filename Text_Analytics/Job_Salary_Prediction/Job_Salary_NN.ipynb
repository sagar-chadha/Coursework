{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Job Salary Prediction using Neural Networks\n",
    "\n",
    "This Kaggle competition challenges us to predict Job salaries based on job ads. The data provided has Job_Title, Location, Category, Contract Type, etc. \n",
    "\n",
    "Lets jump right in - \n",
    "1. Load the libraries required for this task.\n",
    "2. Read in the dataset.\n",
    "3. See what the data looks like.\n",
    "\n",
    "The other jupyter notebook contains the code for predicting job salaries using simple methods such as Naive Bayes, logistic regression and an advanced method - SVM. In this notebook, we will look at solving the problem using a Neural network. This time we will use a much larger sample of the data so that we can effectively train our network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import math\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the train_rev1 datafile downloaded from kaggle\n",
    "df = pd.read_csv('Train_rev1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data has $244,768$ rows with 12 columns, most of which are text type columns as signified by the `object` data type. This means that we will have a large number of (0,1) type variables once the categorical columns are encoded. <br>\n",
    "\n",
    "To be able to use neural networks effective, I will randomly select 80000 rows to do the analysis on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample k rows from the data\n",
    "import random\n",
    "random.seed(1) # so that results are reproducible\n",
    "\n",
    "# get a random sample of k rows from the row indices\n",
    "indices = df.index.values.tolist()\n",
    "random_k = random.sample(indices, 80000)\n",
    "\n",
    "# subset the imported data on the selected 2500 indices\n",
    "train = df.loc[random_k, :]\n",
    "train = train.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some problems with the way FullDescription has been encoded\n",
    "def convert_utf8(s):\n",
    "    return str(s)\n",
    "\n",
    "train['FullDescription'] = train['FullDescription'].map(convert_utf8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean job descriptions\n",
    "\n",
    "**Step 1:** Lets lemmatize the text to remove excess forms of the same word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lemm = WordNetLemmatizer()\n",
    "\n",
    "def convert_to_valid_pos(x):\n",
    "    \n",
    "    x = x[0].upper() # extract first character of the POS tag\n",
    "    \n",
    "    # define mapping for the tag to correct tag.\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "               \"N\": wordnet.NOUN,\n",
    "               \"R\": wordnet.ADV,\n",
    "               \"V\": wordnet.VERB}\n",
    "    \n",
    "    return tag_dict.get(x, wordnet.NOUN)\n",
    "\n",
    "def lemmatize_text(s):\n",
    "    pos_tagged_text = nltk.pos_tag(word_tokenize(s))\n",
    "    \n",
    "    lemm_list = []\n",
    "\n",
    "    for (word, tag) in pos_tagged_text:\n",
    "        lemm_list.append(word_lemm.lemmatize(word, pos = convert_to_valid_pos(tag)))\n",
    "\n",
    "\n",
    "    lemm_text = \" \".join(lemm_list)\n",
    "    return lemm_text\n",
    "\n",
    "train['Full_Description_Lemm'] = train['FullDescription'].map(lemmatize_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using text data is particularly tricky because of the large number of words, numbers, links, symbols, etc in it that is of no value to the prediction problem at hand. We need to manually clean the `FullDescription` column so that it is ready for our analysis. In particular, we will remove urls, numbers, words with stars (hidden characters) and stopwords!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from nltk.corpus import stopwords # store english stopwords in a list\n",
    "en_stopwords = stopwords.words('english')\n",
    "\n",
    "def remove_anomalies(s):\n",
    "    \n",
    "    tokens = word_tokenize(s)\n",
    "    \n",
    "    # urls\n",
    "    weblinks = [w for w in tokens if \".co.uk\" in w] + [w for w in tokens if \".com\" in w] + [w for w in tokens if \"www\" in w]\n",
    "    weblinks = list(set(weblinks)) # remove duplicates from weblinks\n",
    "    \n",
    "    # numbers\n",
    "    numbers = []\n",
    "    for x in tokens:\n",
    "        if len(re.findall('.*[0-9]+.*', x)) > 0:\n",
    "            numbers.append(re.findall('.*[0-9]+.*', x)[0])\n",
    "        else:\n",
    "            numbers.append(np.nan)\n",
    "    \n",
    "    numbers = pd.Series(numbers)\n",
    "    numbers = numbers[~numbers.isnull()].tolist()\n",
    "    \n",
    "    # stars\n",
    "    stars = []\n",
    "    for x in tokens:\n",
    "        if len(re.findall('.*[\\*]+.*', x)) > 0:\n",
    "            stars.append(re.findall('.*[\\*]+,*', x)[0])\n",
    "        else:\n",
    "            stars.append(0)\n",
    "        \n",
    "    stars = pd.Series(stars)   \n",
    "    stars = stars[stars != 0].tolist()\n",
    "    \n",
    "    #stopwords\n",
    "    global en_stopwords\n",
    "    \n",
    "    answer = \" \".join([w for w in tokens if (w not in en_stopwords) & (w not in weblinks) & (w not in numbers) & (w not in stars)])\n",
    "    \n",
    "    for l in punctuation:\n",
    "        answer = answer.replace(l, \"\")\n",
    "    \n",
    "    return answer\n",
    "\n",
    "train['Clean_Full_Descriptions_no_stop'] = train['Full_Description_Lemm'].map(remove_anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Clean_Full_Descriptions_no_stop` has the full descriptions without punctuations, numbers, star words, urls or stopwords!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the target Variable\n",
    "\n",
    "`SalaryNormalized` has the salary values for each job description. We need to create a new categorical variable based off of this that has the value $1$ if salary value is greater than or equal to the $75^{th}$ percentile or $0$ otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the 75th percentile value of salary!\n",
    "sal_perc_75 = np.percentile(train['SalaryNormalized'], 75)\n",
    "\n",
    "# make a new target variable that captures whether salary is high (1) or low (0)\n",
    "train['Salary_Target'] = np.where(train['SalaryNormalized'] >= sal_perc_75, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My modelling approach changes this time. I realised that the approach to create a categorical variable on the basis of 75th percentile of the salary results in an imbalanced dataset. To counter the negative effects of this, I will use a neural network to predict the actual salaries for each job description and then convert them to categorical outputs based on whether they are greater than the 75th percentile of the training salaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the frequencies of words using the TfidfTransformer\n",
    "X = np.array(train.loc[:, 'Clean_Full_Descriptions_no_stop'])\n",
    "y_pxy = np.array(train.loc[:, 'SalaryNormalized'])\n",
    "y_act = np.array(train.loc[:, 'Salary_Target'])\n",
    "\n",
    "# split into test and train data. train_pxy is the continuous variable, train_act is the categorical output that we want.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train_pxy, y_val_pxy = train_test_split(X, y_pxy, test_size = 0.2, random_state = 42)\n",
    "X_train, X_test, y_train_pxy, y_test_pxy = train_test_split(X_train, y_train_pxy, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X_train, X_val, y_train_act, y_val_act = train_test_split(X, y_act, test_size = 0.2, random_state = 42)\n",
    "X_train, X_test, y_train_act, y_test_act = train_test_split(X_train, y_train_act, test_size = 0.2, random_state = 42)\n",
    "\n",
    "# for training to be faster, I will normalize the data as well!\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "std_scaler = StandardScaler()\n",
    "\n",
    "y_train_pxy_scaled = std_scaler.fit_transform(y_train_pxy.reshape(-1, 1))\n",
    "y_val_pxy_scaled = std_scaler.transform(y_val_pxy.reshape(-1, 1))\n",
    "\n",
    "# Convert the arrays into a presence/absence matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 10000)\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_val_counts = count_vectorizer.transform(X_val)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "X_train_nn = np.where(X_train_counts.todense() > 0 , 1, 0)\n",
    "X_val_nn = np.where(X_val_counts.todense() > 0, 1, 0)\n",
    "X_test_nn = np.where(X_test_counts.todense() > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(128, activation = 'relu', input_shape = (10000, )))\n",
    "network.add(layers.Dense(128, activation = 'relu'))\n",
    "network.add(layers.Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the loss and optimizer\n",
    "network.compile(optimizer = 'rmsprop',\n",
    "               loss = 'mse',\n",
    "               metrics = ['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 51200 samples, validate on 16000 samples\n",
      "Epoch 1/10\n",
      "51200/51200 [==============================] - 15s 297us/step - loss: 0.4718 - mean_absolute_error: 0.4762 - val_loss: 0.4101 - val_mean_absolute_error: 0.4224\n",
      "Epoch 2/10\n",
      "51200/51200 [==============================] - 15s 301us/step - loss: 0.2878 - mean_absolute_error: 0.3586 - val_loss: 0.3673 - val_mean_absolute_error: 0.4033\n",
      "Epoch 3/10\n",
      "51200/51200 [==============================] - 15s 302us/step - loss: 0.1743 - mean_absolute_error: 0.2765 - val_loss: 0.3830 - val_mean_absolute_error: 0.4257\n",
      "Epoch 4/10\n",
      "51200/51200 [==============================] - 16s 305us/step - loss: 0.1146 - mean_absolute_error: 0.2247 - val_loss: 0.3768 - val_mean_absolute_error: 0.4126\n",
      "Epoch 5/10\n",
      "51200/51200 [==============================] - 16s 307us/step - loss: 0.0862 - mean_absolute_error: 0.1958 - val_loss: 0.3687 - val_mean_absolute_error: 0.3993\n",
      "Epoch 6/10\n",
      "51200/51200 [==============================] - 16s 307us/step - loss: 0.0713 - mean_absolute_error: 0.1786 - val_loss: 0.3743 - val_mean_absolute_error: 0.4015\n",
      "Epoch 7/10\n",
      "51200/51200 [==============================] - 16s 308us/step - loss: 0.0602 - mean_absolute_error: 0.1644 - val_loss: 0.3703 - val_mean_absolute_error: 0.4053\n",
      "Epoch 8/10\n",
      "51200/51200 [==============================] - 17s 322us/step - loss: 0.0526 - mean_absolute_error: 0.1524 - val_loss: 0.3694 - val_mean_absolute_error: 0.4086\n",
      "Epoch 9/10\n",
      "51200/51200 [==============================] - 16s 311us/step - loss: 0.0465 - mean_absolute_error: 0.1443 - val_loss: 0.3707 - val_mean_absolute_error: 0.3983\n",
      "Epoch 10/10\n",
      "51200/51200 [==============================] - 18s 355us/step - loss: 0.0421 - mean_absolute_error: 0.1365 - val_loss: 0.3622 - val_mean_absolute_error: 0.3918\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "history = network.fit(X_train_nn,\n",
    "           y_train_pxy_scaled,\n",
    "           epochs = 10, \n",
    "           batch_size = 128,\n",
    "           validation_data = (X_val_nn, y_val_pxy_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the categorical outputs\n",
    "From the predicted standardized salaries, we will get back the categorical output and compare to the actual target values to calculate the accuracy! Let's first calculate the validation accuracy and then the test accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.866125"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "predictions = network.predict(X_val_nn)\n",
    "predictions = predictions*np.sqrt(std_scaler.var_) + std_scaler.mean_\n",
    "\n",
    "pred_act = (predictions >= sal_perc_75).astype('int').reshape(-1, )\n",
    "accuracy_score(y_val_act, pred_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8076429132486971"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_val_act, pred_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.862265625"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = network.predict(X_test_nn)\n",
    "predictions = predictions*np.sqrt(std_scaler.var_) + std_scaler.mean_\n",
    "\n",
    "pred_act = (predictions >= sal_perc_75).astype('int').reshape(-1, )\n",
    "accuracy_score(y_test_act, pred_act)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7990948119669864"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test_act, pred_act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "I achieved an accuracy of 86.2% when using just job descriptions with an AUROC score of 0.799 using the neural network approach. This is significantly higher than the value of 83.8% when using SVM. The AUROC score is also higher signifying that this distinguishes between the two classes better!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
